{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "class Config():\n",
    "    # 训练轮次\n",
    "    epoch=10\n",
    "    # 学习率\n",
    "    lr=0.001\n",
    "    min_lr=0.0001\n",
    "    # 训练批次\n",
    "    batch_size=32\n",
    "    # 数据集路径\n",
    "    gen_path='data/GEN-sarc-notsarc.csv'\n",
    "    hyp_path='data/HYP-sarc-notsarc.csv'\n",
    "    rq_path='data/RQ-sarc-notsarc.csv'\n",
    "\n",
    "\n",
    "\n",
    "def judge_data(data):\n",
    "    # 设置1为讽刺\n",
    "    # 0为非讽刺\n",
    "    lable=[0]*len(data)\n",
    "    temp=0\n",
    "    for i in data[\"class\"]:\n",
    "        if i=='sarc':\n",
    "            lable[temp]=1\n",
    "        temp=temp+1    \n",
    "    return lable\n",
    "def load_data():\n",
    "    \n",
    "    config=Config()\n",
    "\n",
    "    # 导入数据 处理数据\n",
    "    d_gen=pd.read_csv(config.gen_path)\n",
    "    d_hyp=pd.read_csv(config.hyp_path)\n",
    "    d_rq=pd.read_csv(config.rq_path)\n",
    "\n",
    "    text1=d_gen[\"text\"]\n",
    "    text2=d_hyp[\"text\"]\n",
    "    text3=d_rq[\"text\"]\n",
    "\n",
    "\n",
    "    # 设置1为讽刺\n",
    "    # 0为非讽刺\n",
    "    lable1=judge_data(d_gen)\n",
    "    lable2=judge_data(d_hyp)\n",
    "    lable3=judge_data(d_rq)\n",
    "\n",
    "    # 合并\n",
    "    texts=[]\n",
    "    labels=[]\n",
    "    texts.extend(text1)\n",
    "    texts.extend(text2)\n",
    "    texts.extend(text3)\n",
    "    labels.extend(lable1)\n",
    "    labels.extend(lable2)\n",
    "    labels.extend(lable3)\n",
    "\n",
    "    return texts,labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer   \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import json  \n",
    "\n",
    "import numpy as np  \n",
    "def creat_token(texts):\n",
    "    # 使用Keras的Tokenizer对句子进行编码\n",
    "    # 初始化 Tokenizer 并拟合文本数据  \n",
    "    tokenizer = Tokenizer(num_words=10000)  # 假设我们只想保留最常见的10000个单词  \n",
    "    tokenizer.fit_on_texts(texts)  \n",
    "\n",
    "    # 将 Tokenizer 配置转换为 JSON 字符串  \n",
    "    tokenizer_config = tokenizer.to_json() \n",
    "\n",
    "    # 将 JSON 字符串保存到文件  \n",
    "    with open('tokenizer_config.json', 'w', encoding='utf-8') as f:  \n",
    "        f.write(tokenizer_config) \n",
    "\n",
    "\n",
    "    # 将文本转换为序列  \n",
    "    sequences = tokenizer.texts_to_sequences(texts)  \n",
    "  \n",
    "    # 填充/截断序列，使其具有相同的长度  \n",
    "    max_len = max([len(seq) for seq in sequences])  \n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')  \n",
    "\n",
    "    return tokenizer,sequences,max_len,padded_sequences\n",
    "\n",
    "def prepare_data(padded_sequences,labels):\n",
    "    # 假设 padded_sequences 是经过填充的序列，labels 是对应的标签  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---load_data---\n",
      "---creat_token---\n",
      "---prepare_data---\n"
     ]
    }
   ],
   "source": [
    "print(\"---load_data---\")\n",
    "texts,labels = load_data()\n",
    "print(\"---creat_token---\")\n",
    "tokenizer,sequences,max_len,padded_sequences = creat_token(texts)\n",
    "print(\"---prepare_data---\")\n",
    "X_train, X_test, y_train, y_test = prepare_data(padded_sequences,lables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看讽刺的语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(labels)):\n",
    "#     if labels[i]==1:\n",
    "#         print('No'+str(i)+':')\n",
    "#         print(texts[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
